{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7341244f-3f51-4248-8443-11bc87add49a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/pytorch_m1/lib/python3.8/site-packages/torchvision/io/image.py:11: UserWarning: Failed to load image Python extension: dlopen(/opt/homebrew/Caskroom/miniforge/base/envs/pytorch_m1/lib/python3.8/site-packages/torchvision/image.so, 0x0006): symbol not found in flat namespace '__ZN2at4_ops19empty_memory_format4callEN3c108ArrayRefIxEENS2_8optionalINS2_10ScalarTypeEEENS5_INS2_6LayoutEEENS5_INS2_6DeviceEEENS5_IbEENS5_INS2_12MemoryFormatEEE'\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import utils.helpers as utils # helper functions\n",
    "\n",
    "import warnings; \n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47cd0cb0-e7fb-4c49-857b-d9483558509b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7285c111-3128-4c67-b4ce-4a9c30e5c1d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[36mGTSRB\u001b[m\u001b[m                           Training.zip\n",
      "GTSRB_Final_Test_Images.zip     \u001b[1m\u001b[36mpbz2\u001b[m\u001b[m\n",
      "GTSRB_Final_Training_Images.zip signnames.csv\n",
      "\u001b[1m\u001b[36mTraining\u001b[m\u001b[m                        traffic-signs-data.zip\n"
     ]
    }
   ],
   "source": [
    "COLAB = 'google.colab' in str(get_ipython())\n",
    "COMPRESSED = True\n",
    "\n",
    "if not COLAB:\n",
    "    import loader.gtsrb_data as dataset\n",
    "    import random\n",
    "    \n",
    "    path = 'data'\n",
    "\n",
    "    if COMPRESSED:\n",
    "        GTSRB_train_dataset = utils.decompress_pickle(path + '/pbz2/GTSRB_train_dataset.pbz2') \n",
    "        GTSRB_train_dataloader = utils.decompress_pickle(path + '/pbz2/GTSRB_train_dataloader.pbz2')\n",
    "    else:\n",
    "        GTSRB_train_dataset = dataset.GTSRB('./data')\n",
    "        GTSRB_train_dataloader = torch.utils.data.DataLoader(GTSRB_train_dataset, \n",
    "                                                  batch_size=128, \n",
    "                                                  shuffle=True, num_workers=2)\n",
    "else:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    path = '/content/drive//MyDrive/DL4CV-2022/GTSRB/'\n",
    "    sys.path.insert(0, '/content/drive/MyDrive/DL4CV-2022/GTSRB/utilities')\n",
    "    \n",
    "!ls $path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23c782f-195b-4e84-a1c1-8dd1c7201dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if COMPRESS:\n",
    "    '''\n",
    "    The pbz2 files below were read in and compressed and pickled using code\n",
    "    adapted from: https://benchmark.ini.rub.de/gtsrb_dataset.html#Codesnippets.\n",
    "    See utils/helpers.py file for details. \n",
    "    '''\n",
    "    GTSRB_train_dataset = utils.decompress_pickle(path + '/pbz2/GTSRB_train_dataset.pbz2') \n",
    "    GTSRB_train_dataloader = utils.decompress_pickle(path + '/pbz2/GTSRB_train_dataloader.pbz2') \n",
    "else:\n",
    "    pass\n",
    "\n",
    "#print ('Train images: ' + str(len(trainImages)) + ', Labels: ' + str(len(trainLabels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6d372d-3944-4217-8c16-f28d320c6bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Datasets\n",
    "import loader.gtsrb_data as dataset\n",
    "import random\n",
    "\n",
    "GTSRB_train_dataset = dataset.GTSRB('./data')\n",
    "GTSRB_train_dataloader = torch.utils.data.DataLoader(GTSRB_train_dataset, \n",
    "                                          batch_size=128, \n",
    "                                          shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54c158b-f73a-4e38-8e8c-004d3d917fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.helpers as utils\n",
    "\n",
    "utils.compress_pickle('GTSRB_train_dataset', GTSRB_train_dataset)\n",
    "utils.compress_pickle('GTSRB_train_dataloader', GTSRB_train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba64d925-7e3c-4e03-aa0a-48f4675d6658",
   "metadata": {},
   "outputs": [],
   "source": [
    "sign_names = utils.make_class_dict(path + '/signnames.csv')\n",
    "classes = list(sign_names.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71217f0-253d-4681-b5ad-9b04983c4db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label, frame = training_set[0]\n",
    "img_frame = list(frame.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e162018-d807-4642-b4e3-83ace6518133",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_frame[3][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70323af-e1ce-4e49-8c9d-5bbcd573898f",
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 3, 3\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(training_set), size=(1,)).item()\n",
    "    img, label, frame = training_set[sample_idx]    \n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(sign_names[label])\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fe3bbf-6ae4-4c51-8ce2-a815310ad91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' GUTTER\n",
    "\n",
    "# for automatic reloading and inline matplotlib\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "#from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "print (len(trainLabels), len(trainImages))\n",
    "\n",
    "\n",
    "class GTSRBLoader(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, data_dir, split, custom_transforms=None, list_dir=None,\n",
    "                 out_name=False,  crop_size=None, num_classes=43, phase=None):\n",
    "        \n",
    "        self.data_dir = data_dir\n",
    "        self.split = split\n",
    "        self.phase = split if phase is None else phase\n",
    "        self.crop_size = 32 if crop_size is None else crop_size\n",
    "        self.out_name = out_name\n",
    "        self.idx2label = idx2label\n",
    "        self.classnames = classnames\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.mean = np.array([0.3337, 0.3064, 0.3171])\n",
    "        self.std = np.array([0.2672, 0.2564, 0.2629])\n",
    "        self.image_list, self.label_list = None, None\n",
    "        self.read_lists()\n",
    "        self.transforms = self.get_transforms(custom_transforms)\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        im = Image.open(f'{self.data_dir}/{self.image_list[index]}')\n",
    "        data = [self.transforms(im)]\n",
    "        data.append(self.label_list[index])\n",
    "        if self.out_name:\n",
    "            data.append(self.image_list[index])\n",
    "        return tuple(data)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_list)\n",
    "\n",
    "\n",
    "    def get_transforms(self, custom_transforms):\n",
    "        if custom_transforms:\n",
    "            return custom_transforms\n",
    "\n",
    "        if 'train' == self.phase:\n",
    "            return transforms.Compose([\n",
    "                transforms.Resize((self.crop_size, self.crop_size)),\n",
    "                transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=self.mean, std=self.std),\n",
    "            ]) \n",
    "        else: \n",
    "            return transforms.Compose([\n",
    "                transforms.Resize((self.crop_size, self.crop_size)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=self.mean, std=self.std),\n",
    "            ])   \n",
    "\n",
    "\n",
    "    def read_lists(self):\n",
    "        image_path = os.path.join(self.data_dir, self.split + '_images.txt')\n",
    "        assert os.path.exists(image_path)\n",
    "        self.image_list = [line.strip().split()[0] for line in open(image_path, 'r')]\n",
    "        self.label_list = [int(line.strip().split()[1]) for line in open(image_path, 'r')]\n",
    "        assert len(self.image_list) == len(self.label_list)\n",
    "\n",
    "    \n",
    "    # get raw image prior to normalization\n",
    "    # expects input image as torch Tensor\n",
    "    def unprocess_image(self, im, plot=False):\n",
    "        im = im.squeeze().numpy().transpose((1, 2, 0))\n",
    "        im = self.std * im + self.mean\n",
    "        im = np.clip(im, 0, 1)\n",
    "        im = im * 255\n",
    "        im = Image.fromarray(im.astype(np.uint8))\n",
    "        \n",
    "        if plot:\n",
    "            plt.imshow(im)\n",
    "            plt.show()\n",
    "        else:\n",
    "            return im\n",
    "  \n",
    "    # de-center images and bring them back to their raw state\n",
    "    def unprocess_batch(self, input):\n",
    "        for i in range(input.size(1)):\n",
    "            input[:,i,:,:] = self.std[i] * input[:,i,:,:]\n",
    "            input[:,i,:,:] = input[:,i,:,:] + self.mean[i]\n",
    "            input[:,i,:,:] = np.clip(input[:,i,:,:], 0, 1)\n",
    "\n",
    "        return input\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7472ca-c0e4-4ab9-bbbb-8f97e3db72b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
