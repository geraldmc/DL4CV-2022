{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8db81a50-bfd1-4fa8-be5b-cc81c0814c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e59d4a65-6227-4a6a-a8fd-7277b6378e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CPU\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "COLAB = 'google.colab' in str(get_ipython())\n",
    "TRANSFORMS = True\n",
    "HAS_GPU = torch.cuda.is_available()\n",
    "is_local = not COLAB\n",
    "\n",
    "if COLAB: # running on Colab\n",
    "    \n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    gdrive_path = '/content/drive//MyDrive/DL4CV-2022/project-I/'\n",
    "    sys.path.append(gdrive_path)\n",
    "\n",
    "    zip_path_train = gdrive_path + 'data/Final_Training.zip'\n",
    "    zip_path_val = gdrive_path + 'data/Final_Validation.zip'\n",
    "    !unzip -q \"{zip_path_train}\"\n",
    "    !unzip -q \"{zip_path_val}\"\n",
    "    \n",
    "    if HAS_GPU:\n",
    "        print(\"Using GPU\")\n",
    "else:\n",
    "    HAS_GPU = False\n",
    "    print(\"Using CPU\")\n",
    "    \n",
    "import utils.helpers as utils\n",
    "import loader.gtsrb_data as dataset\n",
    "\n",
    "# If we run cuda then accomodate these datatypes.\n",
    "FloatTensor = torch.cuda.FloatTensor if HAS_GPU else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if HAS_GPU else torch.LongTensor\n",
    "ByteTensor = torch.cuda.ByteTensor if HAS_GPU else torch.ByteTensor\n",
    "Tensor = FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "070a8a0d-345a-4418-89ba-cf282ca8adf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[36m00000\u001b[m\u001b[m \u001b[1m\u001b[36m00004\u001b[m\u001b[m \u001b[1m\u001b[36m00008\u001b[m\u001b[m \u001b[1m\u001b[36m00012\u001b[m\u001b[m \u001b[1m\u001b[36m00016\u001b[m\u001b[m \u001b[1m\u001b[36m00020\u001b[m\u001b[m \u001b[1m\u001b[36m00024\u001b[m\u001b[m \u001b[1m\u001b[36m00028\u001b[m\u001b[m \u001b[1m\u001b[36m00032\u001b[m\u001b[m \u001b[1m\u001b[36m00036\u001b[m\u001b[m \u001b[1m\u001b[36m00040\u001b[m\u001b[m\n",
      "\u001b[1m\u001b[36m00001\u001b[m\u001b[m \u001b[1m\u001b[36m00005\u001b[m\u001b[m \u001b[1m\u001b[36m00009\u001b[m\u001b[m \u001b[1m\u001b[36m00013\u001b[m\u001b[m \u001b[1m\u001b[36m00017\u001b[m\u001b[m \u001b[1m\u001b[36m00021\u001b[m\u001b[m \u001b[1m\u001b[36m00025\u001b[m\u001b[m \u001b[1m\u001b[36m00029\u001b[m\u001b[m \u001b[1m\u001b[36m00033\u001b[m\u001b[m \u001b[1m\u001b[36m00037\u001b[m\u001b[m \u001b[1m\u001b[36m00041\u001b[m\u001b[m\n",
      "\u001b[1m\u001b[36m00002\u001b[m\u001b[m \u001b[1m\u001b[36m00006\u001b[m\u001b[m \u001b[1m\u001b[36m00010\u001b[m\u001b[m \u001b[1m\u001b[36m00014\u001b[m\u001b[m \u001b[1m\u001b[36m00018\u001b[m\u001b[m \u001b[1m\u001b[36m00022\u001b[m\u001b[m \u001b[1m\u001b[36m00026\u001b[m\u001b[m \u001b[1m\u001b[36m00030\u001b[m\u001b[m \u001b[1m\u001b[36m00034\u001b[m\u001b[m \u001b[1m\u001b[36m00038\u001b[m\u001b[m \u001b[1m\u001b[36m00042\u001b[m\u001b[m\n",
      "\u001b[1m\u001b[36m00003\u001b[m\u001b[m \u001b[1m\u001b[36m00007\u001b[m\u001b[m \u001b[1m\u001b[36m00011\u001b[m\u001b[m \u001b[1m\u001b[36m00015\u001b[m\u001b[m \u001b[1m\u001b[36m00019\u001b[m\u001b[m \u001b[1m\u001b[36m00023\u001b[m\u001b[m \u001b[1m\u001b[36m00027\u001b[m\u001b[m \u001b[1m\u001b[36m00031\u001b[m\u001b[m \u001b[1m\u001b[36m00035\u001b[m\u001b[m \u001b[1m\u001b[36m00039\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "if COLAB:\n",
    "  !ls /content/Final_Training/Images\n",
    "else:\n",
    "    !ls data/Final_Training/Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8abfcf83-83fa-4236-81a1-0e92d4d89196",
   "metadata": {},
   "outputs": [],
   "source": [
    "import loader.args as load_args\n",
    "args = load_args.get_args(is_local)\n",
    "\n",
    "TRAIN_DATA_PATH = args.data_dir_train\n",
    "VAL_DATA_PATH = args.data_dir_val\n",
    "TEST_DATA_PATH = args.data_dir_test\n",
    "BATCH_SIZE = args.batch_size\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = args.batch_size\n",
    "LEARNING_RATE = args.learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abff6254-15ea-4d9c-88f9-aa442cbba976",
   "metadata": {},
   "outputs": [],
   "source": [
    "from loader import transforms as tfs\n",
    "\n",
    "if TRANSFORMS:\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "          torch.utils.data.ConcatDataset([datasets.ImageFolder(\n",
    "                                               TRAIN_DATA_PATH, \n",
    "                                               transform=tfs.data_transforms),\n",
    "                                          datasets.ImageFolder(\n",
    "                                               TRAIN_DATA_PATH,\n",
    "                                               transform=tfs.data_translate),\n",
    "                                          datasets.ImageFolder(\n",
    "                                               TRAIN_DATA_PATH,\n",
    "                                               transform=tfs.data_grayscale),\n",
    "                                          datasets.ImageFolder(\n",
    "                                               TRAIN_DATA_PATH,\n",
    "                                               transform=tfs.data_center),\n",
    "                                          datasets.ImageFolder(\n",
    "                                               TRAIN_DATA_PATH,\n",
    "                                               transform=tfs.data_rotate),\n",
    "                                          datasets.ImageFolder(\n",
    "                                               TRAIN_DATA_PATH,\n",
    "                                               transform=tfs.data_jitter_hue)]), \n",
    "                                               batch_size=args.batch_size, \n",
    "                                               shuffle=True, num_workers=2, \n",
    "                                               pin_memory=HAS_GPU)\n",
    "\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "          torch.utils.data.ConcatDataset([datasets.ImageFolder(\n",
    "                                               VAL_DATA_PATH, \n",
    "                                               transform=tfs.data_transforms),\n",
    "                                          datasets.ImageFolder(\n",
    "                                               VAL_DATA_PATH,\n",
    "                                               transform=tfs.data_translate),\n",
    "                                          datasets.ImageFolder(\n",
    "                                               VAL_DATA_PATH,\n",
    "                                               transform=tfs.data_grayscale),\n",
    "                                          datasets.ImageFolder(\n",
    "                                               VAL_DATA_PATH,\n",
    "                                               transform=tfs.data_center),\n",
    "                                          datasets.ImageFolder(\n",
    "                                               VAL_DATA_PATH,\n",
    "                                               transform=tfs.data_rotate),\n",
    "                                          datasets.ImageFolder(\n",
    "                                               VAL_DATA_PATH,\n",
    "                                               transform=tfs.data_jitter_hue)]), \n",
    "                                               batch_size=args.batch_size, \n",
    "                                               shuffle=False, num_workers=2, \n",
    "                                               pin_memory=HAS_GPU)\n",
    "else:\n",
    "  pass # without transforms the sample number is train=39209, test=12630, val=3870"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6212e598-d64b-4c68-8032-e61cf1d77e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235300\n",
      "23250\n"
     ]
    }
   ],
   "source": [
    "print(len(train_loader)*50)\n",
    "print(len(val_loader)*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21383d32-698e-442c-95d6-ae15ac5476bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmented dataset: 235254 training samples & 23220 validation samples\n",
      "\n",
      "Distribution of classes in augmented train dataset:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "print('Augmented dataset: %d training samples & %d validation samples\\n' % (\n",
    "    len(train_loader.dataset), len(val_loader.dataset)))\n",
    "\n",
    "# Get the labels from concatenated datasets\n",
    "train_dataset_list = train_loader.dataset.datasets\n",
    "train_concat_labels = []\n",
    "for ds in train_dataset_list:\n",
    "    train_concat_labels.extend(ds.targets)\n",
    "    \n",
    "val_dataset_list = val_loader.dataset.datasets\n",
    "val_concat_labels = []\n",
    "for ds in val_dataset_list:\n",
    "    val_concat_labels.extend(ds.targets)\n",
    "\n",
    "class_labels = range(43)\n",
    "\n",
    "print('Distribution of classes in augmented train dataset:')\n",
    "fig, ax = plt.subplots()\n",
    "_, counts = np.unique(train_concat_labels, return_counts=True)\n",
    "ax.bar(class_labels, counts)\n",
    "ax.set_xticks(class_labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef4c9c6-7229-412b-9000-d481c65d6505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network and Optimizer\n",
    "from model.models import Net as NN\n",
    "\n",
    "model = NN()\n",
    "if HAS_GPU:\n",
    "    model.cuda()\n",
    "\n",
    "# Optimizer is updated to *not* include non-gradient weights.    \n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()),\n",
    "                                                      lr=args.learning_rate)\n",
    "# Reduce learning rate when a metric has stopped improving. \n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min',\n",
    "                                                 patience=5,\n",
    "                                                 factor=0.5,\n",
    "                                                 verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c05cef4-43cc-43af-b82b-59fdbbcf365f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    correct = 0\n",
    "    training_loss = 0\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        if HAS_GPU:\n",
    "            data = data.cuda()\n",
    "            target = target.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        max_index = output.max(dim = 1)[1]\n",
    "        correct += (max_index == target).sum()\n",
    "        training_loss += loss\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss per example: {:.6f}\\tLoss: {:.6f}'.format(\n",
    "                        epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                        100. * batch_idx / len(train_loader), loss.data.item()/(args.batch_size * args.log_interval),\n",
    "                        loss.data.item()))\n",
    "            print('\\nTraining set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "                training_loss / len(train_loader.dataset), correct, len(train_loader.dataset),\n",
    "                100. * correct / len(train_loader.dataset)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292c2c5e-2636-414b-bf0a-7a2f181e8b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation():\n",
    "    model.eval()\n",
    "    validation_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in val_loader:\n",
    "        with torch.no_grad():\n",
    "            data, target = Variable(data), Variable(target)\n",
    "            if HAS_GPU:\n",
    "                data = data.cuda()\n",
    "                target = target.cuda()\n",
    "            output = model(data)\n",
    "            validation_loss += F.nll_loss(output, target, size_average=False).data.item() # sum up batch loss\n",
    "            pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    validation_loss /= len(val_loader.dataset)\n",
    "    scheduler.step(np.around(validation_loss,2))\n",
    "    \n",
    "    print('\\nValidation set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        validation_loss, correct, len(val_loader.dataset),\n",
    "        100. * correct / len(val_loader.dataset)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddafcd9-6772-4350-afb8-f25e638968ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, args.num_epochs + 1):\n",
    "    train(epoch)\n",
    "    validation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871b4e85-0c8c-4ebe-8185-ff2189e2a975",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    train_data        = torchvision.datasets.ImageFolder(root=TRAIN_DATA_PATH, \n",
    "                                                         transform=tfs.data_transforms)\n",
    "    train_loader = data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True,  num_workers=2)\n",
    "    test_data         = torchvision.datasets.ImageFolder(root=TEST_DATA_PATH, \n",
    "                                                         transform=tfs.data_transforms)\n",
    "    test_loader  = data.DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "    val_data          = torchvision.datasets.ImageFolder(root=VAL_DATA_PATH, \n",
    "                                                         transform=tfs.data_transforms)\n",
    "    val_loader   = data.DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=True,  num_workers=2)\n",
    "    \n",
    "    \n",
    "ds = datasets.ImageFolder(VAL_DATA_PATH, transform=None)\n",
    "#dir(ds)\n",
    "#ds.samples[0:5] # the first five samples\n",
    "#ds.imgs[33] # img 33\n",
    "#ds.samples[33] # sample 33, equivalent of previous line\n",
    "#ds.samples[33][0] # a sample path\n",
    "#ds.samples[33][1] # a sample class label\n",
    "#ds.find_classes(VAL_DATA_PATH)\n",
    "#ds.targets\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
