{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8db81a50-bfd1-4fa8-be5b-cc81c0814c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "061ab42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatic reloading and inline plotting\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e59d4a65-6227-4a6a-a8fd-7277b6378e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CPU\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "COLAB = 'google.colab' in str(get_ipython())\n",
    "HAS_GPU = torch.cuda.is_available()\n",
    "is_local = not COLAB\n",
    "\n",
    "if COLAB: # running on Colab\n",
    "    \n",
    "    from google.colab import drive\n",
    "    drive.flush_and_unmount()\n",
    "    \n",
    "    drive.mount('/content/drive', force_remount=True)\n",
    "    gdrive_path = '/content/drive//MyDrive/DL4CV-2022/project-I/'\n",
    "    sys.path.append(gdrive_path)\n",
    "\n",
    "    zip_path_train = gdrive_path + 'data/Final_Training.zip'\n",
    "    zip_path_val = gdrive_path + 'data/Final_Validation.zip'\n",
    "    zip_path_test = gdrive_path + 'data/Final_Test.zip'\n",
    "    !unzip -q \"{zip_path_train}\"\n",
    "    !unzip -q \"{zip_path_val}\"\n",
    "    !unzip -q \"{zip_path_test}\"\n",
    "    \n",
    "    if HAS_GPU:\n",
    "        print(\"Using GPU\")\n",
    "else:\n",
    "    HAS_GPU = False\n",
    "    print(\"Using CPU\")\n",
    "    \n",
    "import utils.helpers as utils\n",
    "#import loader.gtsrb_data as dataset\n",
    "\n",
    "# If we run cuda then accomodate these datatypes.\n",
    "FloatTensor = torch.cuda.FloatTensor if HAS_GPU else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if HAS_GPU else torch.LongTensor\n",
    "ByteTensor = torch.cuda.ByteTensor if HAS_GPU else torch.ByteTensor\n",
    "Tensor = FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "070a8a0d-345a-4418-89ba-cf282ca8adf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[36m00000\u001b[m\u001b[m \u001b[1m\u001b[36m00004\u001b[m\u001b[m \u001b[1m\u001b[36m00008\u001b[m\u001b[m \u001b[1m\u001b[36m00012\u001b[m\u001b[m \u001b[1m\u001b[36m00016\u001b[m\u001b[m \u001b[1m\u001b[36m00020\u001b[m\u001b[m \u001b[1m\u001b[36m00024\u001b[m\u001b[m \u001b[1m\u001b[36m00028\u001b[m\u001b[m \u001b[1m\u001b[36m00032\u001b[m\u001b[m \u001b[1m\u001b[36m00036\u001b[m\u001b[m \u001b[1m\u001b[36m00040\u001b[m\u001b[m\n",
      "\u001b[1m\u001b[36m00001\u001b[m\u001b[m \u001b[1m\u001b[36m00005\u001b[m\u001b[m \u001b[1m\u001b[36m00009\u001b[m\u001b[m \u001b[1m\u001b[36m00013\u001b[m\u001b[m \u001b[1m\u001b[36m00017\u001b[m\u001b[m \u001b[1m\u001b[36m00021\u001b[m\u001b[m \u001b[1m\u001b[36m00025\u001b[m\u001b[m \u001b[1m\u001b[36m00029\u001b[m\u001b[m \u001b[1m\u001b[36m00033\u001b[m\u001b[m \u001b[1m\u001b[36m00037\u001b[m\u001b[m \u001b[1m\u001b[36m00041\u001b[m\u001b[m\n",
      "\u001b[1m\u001b[36m00002\u001b[m\u001b[m \u001b[1m\u001b[36m00006\u001b[m\u001b[m \u001b[1m\u001b[36m00010\u001b[m\u001b[m \u001b[1m\u001b[36m00014\u001b[m\u001b[m \u001b[1m\u001b[36m00018\u001b[m\u001b[m \u001b[1m\u001b[36m00022\u001b[m\u001b[m \u001b[1m\u001b[36m00026\u001b[m\u001b[m \u001b[1m\u001b[36m00030\u001b[m\u001b[m \u001b[1m\u001b[36m00034\u001b[m\u001b[m \u001b[1m\u001b[36m00038\u001b[m\u001b[m \u001b[1m\u001b[36m00042\u001b[m\u001b[m\n",
      "\u001b[1m\u001b[36m00003\u001b[m\u001b[m \u001b[1m\u001b[36m00007\u001b[m\u001b[m \u001b[1m\u001b[36m00011\u001b[m\u001b[m \u001b[1m\u001b[36m00015\u001b[m\u001b[m \u001b[1m\u001b[36m00019\u001b[m\u001b[m \u001b[1m\u001b[36m00023\u001b[m\u001b[m \u001b[1m\u001b[36m00027\u001b[m\u001b[m \u001b[1m\u001b[36m00031\u001b[m\u001b[m \u001b[1m\u001b[36m00035\u001b[m\u001b[m \u001b[1m\u001b[36m00039\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "if COLAB:\n",
    "  !ls /content/Final_Training/Images\n",
    "else:\n",
    "  !ls data/Final_Training/Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8abfcf83-83fa-4236-81a1-0e92d4d89196",
   "metadata": {},
   "outputs": [],
   "source": [
    "import loader.args as load_args\n",
    "args = load_args.get_args(is_local)\n",
    "\n",
    "TRAIN_DATA_PATH = args.data_dir_train\n",
    "VAL_DATA_PATH = args.data_dir_val\n",
    "TEST_DATA_PATH = args.data_dir_test\n",
    "BATCH_SIZE = args.batch_size\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = args.batch_size\n",
    "LEARNING_RATE = args.learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abff6254-15ea-4d9c-88f9-aa442cbba976",
   "metadata": {},
   "outputs": [],
   "source": [
    "from loader import transforms as tfs\n",
    "    # NO augmentation--> train=39209, test=12630, val=3870.\n",
    "\n",
    "train_data = datasets.ImageFolder(\n",
    "    root=TRAIN_DATA_PATH, \n",
    "    transform=tfs.base_transform)\n",
    "train_loader = data.DataLoader(train_data, \n",
    "    batch_size=BATCH_SIZE, shuffle=True,  \n",
    "    num_workers=2)\n",
    "\n",
    "val_data = datasets.ImageFolder(\n",
    "    root=VAL_DATA_PATH, \n",
    "    transform=tfs.base_transform)\n",
    "val_loader  = data.DataLoader(val_data, \n",
    "    batch_size=BATCH_SIZE, shuffle=False, \n",
    "    num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21383d32-698e-442c-95d6-ae15ac5476bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unaugmented dataset: 39250 training samples & 3900 validation samples\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "print('Unaugmented dataset: %d training samples & %d validation samples\\n' % (\n",
    "len(train_loader)*args.batch_size, len(val_loader)*args.batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9c54db2-1828-4d6e-bad4-02304127f857",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.num_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ef4c9c6-7229-412b-9000-d481c65d6505",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.models import Net1\n",
    "\n",
    "model = Net1()\n",
    "if HAS_GPU:\n",
    "    model.cuda()\n",
    "\n",
    "# Optimizer is updated to *not* include non-gradient weights.    \n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()),\n",
    "                                                      lr=args.learning_rate)\n",
    "# Reduce learning rate when a metric has stopped improving. \n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min',\n",
    "                                                 patience=5,\n",
    "                                                 factor=0.5,\n",
    "                                                 verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f585db72-db78-4fee-b18a-67aaef33de9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [-1, 8, 26, 26]           1,184\n",
      "         MaxPool2d-2            [-1, 8, 13, 13]               0\n",
      "              ReLU-3            [-1, 8, 13, 13]               0\n",
      "            Conv2d-4             [-1, 10, 9, 9]           2,010\n",
      "         MaxPool2d-5             [-1, 10, 4, 4]               0\n",
      "              ReLU-6             [-1, 10, 4, 4]               0\n",
      "            Linear-7                   [-1, 32]           5,152\n",
      "              ReLU-8                   [-1, 32]               0\n",
      "            Linear-9                    [-1, 6]             198\n",
      "           Conv2d-10          [-1, 100, 28, 28]           7,600\n",
      "      BatchNorm2d-11          [-1, 100, 14, 14]             200\n",
      "        Dropout2d-12          [-1, 100, 14, 14]               0\n",
      "           Conv2d-13          [-1, 150, 12, 12]         135,150\n",
      "      BatchNorm2d-14            [-1, 150, 6, 6]             300\n",
      "        Dropout2d-15            [-1, 150, 6, 6]               0\n",
      "           Conv2d-16            [-1, 250, 4, 4]         337,750\n",
      "      BatchNorm2d-17            [-1, 250, 2, 2]             500\n",
      "        Dropout2d-18            [-1, 250, 2, 2]               0\n",
      "           Linear-19                  [-1, 350]         350,350\n",
      "           Linear-20                   [-1, 43]          15,093\n",
      "================================================================\n",
      "Total params: 855,487\n",
      "Trainable params: 855,487\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 1.26\n",
      "Params size (MB): 3.26\n",
      "Estimated Total Size (MB): 4.54\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model,(3,32,32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c05cef4-43cc-43af-b82b-59fdbbcf365f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.meter import AvgMeter, calc_accuracy\n",
    "\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "\n",
    "def train(epoch):\n",
    "    print('Epoch {} -------------------------------------------------------->'.format(epoch) )\n",
    "    print()\n",
    "    losses = AvgMeter()\n",
    "    accuracies = AvgMeter()\n",
    "    model.train()\n",
    "    correct = 0\n",
    "    training_loss = 0\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        if HAS_GPU:\n",
    "            data = data.cuda()\n",
    "            target = target.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        \n",
    "        losses.update(loss.item(), data.size(0))\n",
    "        accuracy = calc_accuracy(output, target)[0]\n",
    "        train_accuracies.append(int(accuracy))\n",
    "        accuracies.update(accuracy, data.size(0))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        max_index = output.max(dim = 1)[1] # _, predicted\n",
    "        correct += (max_index == target).sum()\n",
    "        training_loss += loss\n",
    "        \n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Training set: Average loss: {:.4f}, Accuracy: {}/{} ({:.3f}%)\\n'.format(\n",
    "            losses.avg, int(accuracies.sum/100), accuracies.count, accuracies.avg))\n",
    "    \n",
    "    return train_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "292c2c5e-2636-414b-bf0a-7a2f181e8b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "def validate(epoch):\n",
    "    losses = AvgMeter()\n",
    "    accuracies = AvgMeter()\n",
    "    error_cases = []\n",
    "    \n",
    "    model.eval() # changes the forward() behaviour of the module for val/test.\n",
    "    \n",
    "    with torch.no_grad(): # disable tracking of gradients in autograd.\n",
    "        for data, target in val_loader:\n",
    "            if HAS_GPU:\n",
    "                data = data.cuda()\n",
    "                target = target.cuda()\n",
    "            output = model(data)\n",
    "            loss = F.nll_loss(output, target)\n",
    "\n",
    "            losses.update(loss.item(), data.size(0))\n",
    "            accuracy = calc_accuracy(output, target)[0]\n",
    "            val_accuracies.append(accuracy)\n",
    "            accuracies.update(accuracy, data.size(0))\n",
    "\n",
    "            _, pred = output.topk(1, 1, True, True)\n",
    "            pred = pred.t()\n",
    "\n",
    "        print('\\nValidation set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "            losses.avg, int(accuracies.sum/100), accuracies.count, accuracies.avg))\n",
    "\n",
    "    return val_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f69203a-a1a8-487b-a64e-294519880ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.metrics as sm\n",
    "\n",
    "def test(test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    targets, preds = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            if HAS_GPU:\n",
    "                data = data.cuda()\n",
    "                target = target.cuda()\n",
    "            output = model(data)\n",
    "            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "            targets += list(target.cpu().numpy())\n",
    "            preds += list(pred.cpu().numpy())\n",
    "\n",
    "    test_acc = 100. * correct / len(test_loader.dataset)\n",
    "    confusion_mtx = sm.confusion_matrix(targets, preds)\n",
    "    return test_acc, confusion_mtx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ddafcd9-6772-4350-afb8-f25e638968ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 -------------------------------------------------------->\n",
      "\n",
      "Training set: Average loss: 3.8708, Accuracy: 2/50 (4.000%)\n",
      "\n",
      "Training set: Average loss: 3.4279, Accuracy: 1146/10050 (11.403%)\n",
      "\n",
      "Training set: Average loss: 3.0673, Accuracy: 3988/20050 (19.890%)\n",
      "\n",
      "Training set: Average loss: 2.7199, Accuracy: 8349/30050 (27.784%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/pytorch_m1/lib/python3.8/site-packages/torchvision/io/image.py:11: UserWarning: Failed to load image Python extension: dlopen(/opt/homebrew/Caskroom/miniforge/base/envs/pytorch_m1/lib/python3.8/site-packages/torchvision/image.so, 0x0006): symbol not found in flat namespace '__ZN2at4_ops19empty_memory_format4callEN3c108ArrayRefIxEENS2_8optionalINS2_10ScalarTypeEEENS5_INS2_6LayoutEEENS5_INS2_6DeviceEEENS5_IbEENS5_INS2_12MemoryFormatEEE'\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/pytorch_m1/lib/python3.8/site-packages/torchvision/io/image.py:11: UserWarning: Failed to load image Python extension: dlopen(/opt/homebrew/Caskroom/miniforge/base/envs/pytorch_m1/lib/python3.8/site-packages/torchvision/image.so, 0x0006): symbol not found in flat namespace '__ZN2at4_ops19empty_memory_format4callEN3c108ArrayRefIxEENS2_8optionalINS2_10ScalarTypeEEENS5_INS2_6LayoutEEENS5_INS2_6DeviceEEENS5_IbEENS5_INS2_12MemoryFormatEEE'\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/pytorch_m1/lib/python3.8/site-packages/torchvision/io/image.py:11: UserWarning: Failed to load image Python extension: dlopen(/opt/homebrew/Caskroom/miniforge/base/envs/pytorch_m1/lib/python3.8/site-packages/torchvision/image.so, 0x0006): symbol not found in flat namespace '__ZN2at4_ops19empty_memory_format4callEN3c108ArrayRefIxEENS2_8optionalINS2_10ScalarTypeEEENS5_INS2_6LayoutEEENS5_INS2_6DeviceEEENS5_IbEENS5_INS2_12MemoryFormatEEE'\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/pytorch_m1/lib/python3.8/site-packages/torchvision/io/image.py:11: UserWarning: Failed to load image Python extension: dlopen(/opt/homebrew/Caskroom/miniforge/base/envs/pytorch_m1/lib/python3.8/site-packages/torchvision/image.so, 0x0006): symbol not found in flat namespace '__ZN2at4_ops19empty_memory_format4callEN3c108ArrayRefIxEENS2_8optionalINS2_10ScalarTypeEEENS5_INS2_6LayoutEEENS5_INS2_6DeviceEEENS5_IbEENS5_INS2_12MemoryFormatEEE'\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation set: Average loss: 1.5327, Accuracy: 2181/3870 (56.36%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_accuracy = []\n",
    "validation_accuracy = []\n",
    "\n",
    "#for epoch in range(1, args.num_epochs + 1):\n",
    "for epoch in range(1, 1 + 1):\n",
    "    training_accuracy.append(train(epoch))\n",
    "    validation_accuracy.append(validate(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b4359fa-ad17-4308-bd14-5c334bb8566e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot accuracy\n",
    "\n",
    "plt.plot(training_accuracy,'-o', markersize=.5)\n",
    "plt.plot(validation_accuracy,'-o', markersize=.5)\n",
    "plt.xlabel('Sample # per Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(['Train','Valid'])\n",
    "plt.title('Train vs Valid Accuracy per Epoch (Best)')\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a51a14f-0cd8-442e-9a36-d8998501d421",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02829ed-9671-46d6-9597-c15c701cce8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05925f9-ed14-4c88-a89c-7ed0776e6a70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786cb7d3-0136-491e-820f-6b6d67104afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from loader.gtsrb_data import GTSRB_Test\n",
    "from utils.tensor2image import imshow\n",
    "\n",
    "if COLAB:\n",
    "    GTSRB_test_dataset = GTSRB_Test('/content/')\n",
    "    \n",
    "else:\n",
    "    GTSRB_test_dataset = GTSRB_Test('data/')\n",
    "\n",
    "assert len(GTSRB_test_dataset) == 12630"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206601d0-5180-4af6-a443-697023111719",
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = GTSRB_test_dataset[7]\n",
    "imshow(img, label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1109c5e8-afba-4f81-b994-307efb8117b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "GTSRB_test_dataset.csv_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20544779-0c74-4fb1-9cd7-c054b31302f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader  = data.DataLoader(GTSRB_test_dataset, \n",
    "    batch_size=BATCH_SIZE, shuffle=False, \n",
    "    num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2e4584-f4a7-4481-a29a-706567b15bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accs, confusion_mtxes = [], []\n",
    "\n",
    "test_acc, confusion_mtx = test(test_loader)\n",
    "test_accs.append(test_acc)\n",
    "confusion_mtxes.append(confusion_mtx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454593b3-ce38-457f-bcf1-5ae3013a214f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\rBest test acc = %2.2f%%' % max(test_accs), end='', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2633357d-e225-42c7-b7df-4daa4d99a564",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b076fabd-7217-4604-9ebf-b8bfaadc5cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc, confusion_mtx = test(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ccff42-846b-4488-8a8e-6f2bdecd8774",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accs, confusion_mtxes = [], []\n",
    "test_accs.append(test_acc)\n",
    "confusion_mtxes.append(confusion_mtx)\n",
    "print('\\rBest test acc = %2.2f%%' % max(test_accs), end='', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea70f8c-7df9-4267-8481-b94c52097c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edfd837-e0da-4e93-b28f-ff97f4e1fb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "print('Dataset: %d training samples & %d testing samples\\n' % (\n",
    "    len(train_loader.dataset), len(test_loader.dataset)))\n",
    " \n",
    "print('Distribution of classes in dataset:')\n",
    "fig, ax = plt.subplots()\n",
    "labels = [label for _, label in train_loader.dataset.imgs]\n",
    "class_labels, counts = np.unique(labels, return_counts=True)\n",
    "ax.bar(class_labels, counts)\n",
    "ax.set_xticks(class_labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e54c33a-805b-4397-b36e-83d4e7e2d4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.visualize import visual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f85629-f16a-4bcf-99bc-a9999945ab4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "visual(test_accs, confusion_mtxes, classe_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bbadaf-5400-44a7-9f0b-6dcd303e63db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc5ee05-e3b5-45c7-96ae-74a12231c184",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dc3b93-f2c5-469f-867c-a7368282941b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c5c7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot accuracy\n",
    "\n",
    "plt.plot(training_accuracy,'-o', markersize=.5)\n",
    "plt.plot(validation_accuracy,'-o', markersize=.5)\n",
    "plt.xlabel('Sample # per Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(['Train','Valid'])\n",
    "plt.title('Train vs Valid Accuracy per Epoch (Best)')\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3efedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = None\n",
    "val_loader = None\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    " torch.utils.data.ConcatDataset([datasets.ImageFolder(\n",
    "                                    TRAIN_DATA_PATH, \n",
    "                                    transform=tfs.base_transform),\n",
    "                               datasets.ImageFolder(\n",
    "                                    TRAIN_DATA_PATH,\n",
    "                                    transform=tfs.data_translate),\n",
    "                               datasets.ImageFolder(\n",
    "                                    TRAIN_DATA_PATH,\n",
    "                                    transform=tfs.data_grayscale),\n",
    "                               datasets.ImageFolder(\n",
    "                                    TRAIN_DATA_PATH,\n",
    "                                    transform=tfs.data_center),\n",
    "                               datasets.ImageFolder(\n",
    "                                    TRAIN_DATA_PATH,\n",
    "                                    transform=tfs.data_rotate),\n",
    "                               datasets.ImageFolder(\n",
    "                                    TRAIN_DATA_PATH,\n",
    "                                    transform=tfs.data_jitter_hue)]), \n",
    "                                    batch_size=args.batch_size, \n",
    "                                    shuffle=True, num_workers=2, \n",
    "                                    pin_memory=HAS_GPU)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    " torch.utils.data.ConcatDataset([datasets.ImageFolder(\n",
    "                                    VAL_DATA_PATH, \n",
    "                                    transform=tfs.base_transform),\n",
    "                               datasets.ImageFolder(\n",
    "                                    VAL_DATA_PATH,\n",
    "                                    transform=tfs.data_translate),\n",
    "                               datasets.ImageFolder(\n",
    "                                    VAL_DATA_PATH,\n",
    "                                    transform=tfs.data_grayscale),\n",
    "                               datasets.ImageFolder(\n",
    "                                    VAL_DATA_PATH,\n",
    "                                    transform=tfs.data_center),\n",
    "                               datasets.ImageFolder(\n",
    "                                    VAL_DATA_PATH,\n",
    "                                    transform=tfs.data_rotate),\n",
    "                               datasets.ImageFolder(\n",
    "                                    VAL_DATA_PATH,\n",
    "                                    transform=tfs.data_jitter_hue)]), \n",
    "                                    batch_size=args.batch_size, \n",
    "                                    shuffle=False, num_workers=2, \n",
    "                                    pin_memory=HAS_GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a228991",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Augmented dataset: %d training samples & %d validation samples\\n' % (\n",
    "    len(train_loader.dataset), len(val_loader.dataset)))\n",
    "\n",
    "# Get labels from concatenated datasets.\n",
    "train_dataset_list = train_loader.dataset.datasets\n",
    "train_concat_labels = []\n",
    "for ds in train_dataset_list:\n",
    "    train_concat_labels.extend(ds.targets)\n",
    "\n",
    "val_dataset_list = val_loader.dataset.datasets\n",
    "val_concat_labels = []\n",
    "for ds in val_dataset_list:\n",
    "    val_concat_labels.extend(ds.targets)\n",
    "\n",
    "class_labels = range(43)\n",
    "\n",
    "print('Distribution of classes in augmented train dataset:')\n",
    "fig, ax = plt.subplots()\n",
    "_, counts = np.unique(train_concat_labels, return_counts=True)\n",
    "ax.bar(class_labels, counts)\n",
    "ax.set_xticks(class_labels, minor=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34377311",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None\n",
    "\n",
    "model = Net1()\n",
    "if HAS_GPU:\n",
    "    model.cuda()\n",
    "\n",
    "# Optimizer is updated to *not* include non-gradient weights.    \n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()),\n",
    "                                                      lr=args.learning_rate)\n",
    "# Reduce learning rate when a metric has stopped improving. \n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min',\n",
    "                                                 patience=5,\n",
    "                                                 factor=0.5,\n",
    "                                                 verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb38f062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset these\n",
    "training_accuracy = []\n",
    "validation_accuracy = []\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "# Note: Runtime for the following is roughly 8:35.\n",
    "for epoch in range(1, args.num_epochs + 1):\n",
    "    training_accuracy.append(train(epoch))\n",
    "    validation_accuracy.append(validate(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ee3b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy\n",
    "\n",
    "plt.plot(training_accuracy[4],'-o', markersize=.5)\n",
    "plt.plot(validation_accuracy[4],'-o', markersize=.5)\n",
    "plt.xlabel('Sample # per Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(['Train','Valid'])\n",
    "plt.title('Train vs Valid Accuracy per Epoch (Best)')\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275e60c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86036ca5-87b9-4e5d-ac54-f7aa79039219",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88ca5bc-a7f8-40e1-a64a-89f5f2f5f3a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf76ad96-99c8-4b09-ae28-9e75258630c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871b4e85-0c8c-4ebe-8185-ff2189e2a975",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    train_data        = torchvision.datasets.ImageFolder(root=TRAIN_DATA_PATH, \n",
    "                                                         transform=tfs.data_transforms)\n",
    "    train_loader = data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True,  num_workers=2)\n",
    "    test_data         = torchvision.datasets.ImageFolder(root=TEST_DATA_PATH, \n",
    "                                                         transform=tfs.data_transforms)\n",
    "    test_loader  = data.DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "    val_data          = torchvision.datasets.ImageFolder(root=VAL_DATA_PATH, \n",
    "                                                         transform=tfs.data_transforms)\n",
    "    val_loader   = data.DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=True,  num_workers=2)\n",
    "    \n",
    "    \n",
    "ds = datasets.ImageFolder(VAL_DATA_PATH, transform=None)\n",
    "#dir(ds)\n",
    "#ds.samples[0:5] # the first five samples\n",
    "#ds.imgs[33] # img 33\n",
    "#ds.samples[33] # sample 33, equivalent of previous line\n",
    "#ds.samples[33][0] # a sample path\n",
    "#ds.samples[33][1] # a sample class label\n",
    "#ds.find_classes(VAL_DATA_PATH)\n",
    "#ds.targets\n",
    "\n",
    "\n",
    "def validation():\n",
    "    model.eval()\n",
    "    validation_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in val_loader:\n",
    "        with torch.no_grad():\n",
    "            data, target = Variable(data), Variable(target)\n",
    "            if HAS_GPU:\n",
    "                data = data.cuda()\n",
    "                target = target.cuda()\n",
    "            output = model(data)\n",
    "            validation_loss += F.nll_loss(output, target, size_average=False).data.item() # sum up batch loss\n",
    "            pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    validation_loss /= len(val_loader.dataset)\n",
    "    scheduler.step(np.around(validation_loss,2))\n",
    "    \n",
    "    print('\\nValidation set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        validation_loss, correct, len(val_loader.dataset),\n",
    "        100. * correct / len(val_loader.dataset))) \n",
    "        \n",
    "        \n",
    "def train(epoch):\n",
    "    losses = AvgMeter()\n",
    "    accuracies = AvgMeter()\n",
    "    \n",
    "    model.train()\n",
    "    correct = 0\n",
    "    training_loss = 0\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        if HAS_GPU:\n",
    "            data = data.cuda()\n",
    "            target = target.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        \n",
    "        #losses.update(loss.item(), data.size(0))\n",
    "        #accuracy = calc_accuracy(output, target)[0]\n",
    "        #accuracies.update(accuracy, data.size(0))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        max_index = output.max(dim = 1)[1] # _, predicted\n",
    "        correct += (max_index == target).sum()\n",
    "        training_loss += loss\n",
    "        \n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            train_loss=training_loss / len(train_loader.dataset)\n",
    "            accu = 100. * correct / len(train_loader.dataset)\n",
    "            print('Train Loss: %.3f | Accuracy: %.3f'%(train_loss,accu))\n",
    "            print('---------------------------------------------------')\n",
    "            print('Epoch: {} [{}/{} ({:.0f}%)]\\tLoss per example: {:.6f}\\tLoss: {:.6f}'.format(\n",
    "                        epoch, \n",
    "                        batch_idx * len(data), # this batch number, times total length\n",
    "                        len(train_loader.dataset), # length of the entire dataset.\n",
    "                        100. * batch_idx / len(train_loader), \n",
    "                        loss.data.item()/(args.batch_size * args.log_interval),\n",
    "                        loss.data.item()) )\n",
    "            \n",
    "    train_accu.append(accu.detach().cpu().numpy())\n",
    "    train_losses.append(train_loss.detach().cpu().numpy())\n",
    "    \n",
    "    \n",
    "def validation():\n",
    "    eval_losses=[]\n",
    "    eval_accu_ls=[]\n",
    "\n",
    "    model.eval()\n",
    "    validation_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in val_loader:\n",
    "        with torch.no_grad():\n",
    "            data, target = Variable(data), Variable(target)\n",
    "            if HAS_GPU:\n",
    "                data = data.cuda()\n",
    "                target = target.cuda()\n",
    "            output = model(data)\n",
    "             # sum up batch loss\n",
    "            validation_loss += F.nll_loss(output, \n",
    "                                          target, \n",
    "                                          size_average=False).data.item()\n",
    "            # get the index of the max log-probability\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "        validation_loss /= len(val_loader.dataset)\n",
    "        scheduler.step(np.around(validation_loss,2))\n",
    "\n",
    "        eval_loss=validation_loss / len(val_loader.dataset)\n",
    "        eval_accu = 100. * correct / len(val_loader.dataset)\n",
    "\n",
    "    eval_losses.append(eval_loss.detach().cpu().numpy())\n",
    "    eval_accu_ls.append(eval_accu.detach().cpu().numpy())\n",
    "\n",
    "    print('Validation Loss: %.3f | Accuracy: %.3f'%(test_loss,accu)) \n",
    "\n",
    "    print('\\nValidation set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        validation_loss, \n",
    "        correct, \n",
    "        len(val_loader.dataset),\n",
    "        100. * correct / len(val_loader.dataset))\n",
    "        \n",
    "        \n",
    "test_data = datasets.ImageFolder(\n",
    "    root=TEST_DATA_PATH, \n",
    "    transform=tfs.base_transform)\n",
    "test_loader  = data.DataLoader(test_data, \n",
    "    batch_size=BATCH_SIZE, shuffle=True, \n",
    "    num_workers=2)\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
